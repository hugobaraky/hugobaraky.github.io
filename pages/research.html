<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research - Your Name</title>
    
    <!-- MathJax Configuration -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    
    <!-- Highlight.js CSS (Optional, for future code blocks) -->
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    
    <!-- Highlight.js JS (Optional, for future code blocks) -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    
    <!-- Initialize Highlight.js (Optional, for future code blocks) -->
    <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            document.querySelectorAll('pre code').forEach((block) => {
                hljs.highlightElement(block);
            });
        });
    </script>
    
    <!-- Inline CSS for Styling -->
    <style>
        /* Sidebar Navigation Styling */
        nav {
            width: 200px;
            background-color: #333;
            color: #fff;
            padding-top: 60px;
            position: fixed;
            left: 0;
            top: 0;
            height: 100%;
        }

        nav ul {
            list-style-type: none;
            padding: 0;
        }

        nav ul li {
            margin: 20px 0;
            text-align: center;
        }

        nav ul li a {
            color: #fff;
            text-decoration: none;
            font-size: 18px;
            display: block;
            padding: 10px;
            transition: background 0.3s;
        }

        nav ul li a:hover,
        nav ul li a.active {
            background-color: #575757;
            border-radius: 4px;
        }

        /* Main Content Styling */
        .main-content {
            flex: 1;
            padding: 20px;
            margin-left: 200px;
            max-width: 800px;
        }

        /* Footer Styling */
        footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 10px;
            position: fixed;
            width: 100%;
            bottom: 0;
            left: 0;
        }

        /* Responsive Adjustments */
        @media (max-width: 768px) {
            nav {
                width: 100%;
                position: relative;
                height: auto;
            }
            .main-content {
                margin-left: 0;
            }
        }

        /* Section Styling */
        section {
            margin-bottom: 40px;
        }

        /* Heading Styling */
        h1, h2, h3, h4 {
            color: #333;
        }

        /* Paragraph Styling */
        p {
            line-height: 1.6;
            color: #555;
        }

        /* List Styling */
        ul {
            margin-left: 20px;
            color: #555;
        }

        /* Emphasis Styling */
        em {
            color: #000;
        }

        /* Strong Styling */
        strong {
            color: #000;
        }
    </style>
</head>
<body style="font-family: Arial, sans-serif; display: flex; min-height: 100vh; margin: 0; padding: 0;">

    <!-- Sidebar Navigation -->
    <nav>
        <ul>
            <li>
                <a href="index.html">Introduction</a>
            </li>
            <li>
                <a href="experience.html">Experience</a>
            </li>
            <li>
                <a href="research.html" class="active">Research</a>
            </li>
            <li>
                <a href="meuzapcontrole.html" class="active">Meu Zap Controle</a>
            </li>
        </ul>
    </nav>

    <!-- Main Content -->
    <div class="main-content">
        <h1>Research</h1>

        <section>
            <h2>SmoothGrad vs. Expected Gradients in Shapley Value Estimation</h2>
            <p>
                In my M.Sc. thesis, I explore the effectiveness of <strong>SmoothGrad</strong> compared to <strong>Expected Gradients</strong> when utilizing Shapley values for feature attribution in deep learning models. The primary objective is to demonstrate that SmoothGrad provides more reliable and interpretable explanations for model predictions.
            </p>
        </section>

        <section>
            <h3>Introduction to Shapley Values</h3>
            <p>
                <strong>Shapley values</strong> originate from cooperative game theory and provide a principled way to attribute the contribution of each feature to the overall prediction of a model. By considering all possible feature combinations, Shapley values ensure fairness and consistency in feature attribution, making them a popular choice for interpretability in machine learning.
            </p>
            <p>
                Mathematically, the Shapley value for a feature \( i \) is defined as:
            </p>
            <p>
                $$
                \phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|! \cdot (|N| - |S| - 1)!}{|N|!} \left[ v(S \cup \{i\}) - v(S) \right]
                $$
            </p>
            <p>
                Where:
            </p>
            <ul>
                <li><strong> \( N \)</strong> is the set of all features.</li>
                <li><strong> \( S \)</strong> is a subset of features excluding feature \( i \).</li>
                <li><strong> \( v(S) \)</strong> is the value function, representing the model's prediction when only features in \( S \) are present.</li>
            </ul>
        </section>

        <section>
            <h3>Expected Gradients</h3>
            <p>
                <strong>Expected Gradients</strong> is an extension of the <strong>Integrated Gradients (IG)</strong> method, designed to approximate Shapley values for feature attribution in differentiable models. Expected Gradients reformulates the IG integral as an expectation over a distribution of reference inputs, thereby aligning more closely with the Shapley value framework.
            </p>
            <p>
                The Integrated Gradients for the \( i^{th} \) input feature is defined as:
            </p>
            <p>
                $$
                IG_i(x) = (x_i - x'_i) \times \int_{\alpha=0}^{1} \frac{\partial F(x' + \alpha \times (x - x'))}{\partial x_i} d\alpha
                $$
            </p>
            <p>
                Where:
            </p>
            <ul>
                <li><strong> \( x \)</strong> is the input vector.</li>
                <li><strong> \( x' \)</strong> is the baseline input (e.g., all zeros).</li>
                <li><strong> \( F \)</strong> is the model's output function.</li>
                <li><strong> \( \alpha \)</strong> is a scalar that interpolates between the baseline and the input.</li>
            </ul>
            <p>
                To approximate the integral, Expected Gradients uses a discrete summation:
            </p>
            <p>
                $$
                IG_i(x) \approx (x_i - x'_i) \times \frac{1}{m} \sum_{k=1}^{m} \frac{\partial F(x'_k)}{\partial x_i}
                $$
            </p>
            <p>
                Where \( m \) is the number of steps in the approximation, and \( x'_k \) are sampled reference inputs from the background dataset.
            </p>
        </section>

        <section>
            <h3>SmoothGrad</h3>
            <p>
                <strong>SmoothGrad</strong> is a method designed to improve the clarity and reliability of gradient-based feature attributions. By averaging gradients over multiple noisy copies of the input, SmoothGrad reduces the inherent noise in gradient estimates, leading to smoother and more interpretable attribution maps.
            </p>
            <p>
                Mathematically, SmoothGrad computes the attributions as:
            </p>
            <p>
                $$
                SG_i(x) = \frac{1}{m} \sum_{k=1}^{m} \nabla_i F(x + \mathcal{N}_k)
                $$
            </p>
            <p>
                Where:
            </p>
            <ul>
                <li><strong> \( \mathcal{N}_k \)</strong> represents noise added to the input \( x \).</li>
                <li><strong> \( m \)</strong> is the number of noisy samples.</li>
                <li><strong> \( \nabla_i F \) </strong> is the gradient of the model's output with respect to the \( i^{th} \) input feature.</li>
            </ul>
            <p>
                By aggregating gradients across noisy samples, SmoothGrad emphasizes the most salient features while diminishing the impact of noisy, less important gradients.
            </p>
        </section>

        <section>
            <h3>Comparative Analysis: SmoothGrad vs. Expected Gradients</h3>
            <p>
                Both <strong>SmoothGrad</strong> and <strong>Expected Gradients</strong> aim to enhance gradient-based feature attribution methods. However, SmoothGrad offers several advantages over Expected Gradients, particularly in the context of approximating Shapley values.
            </p>

            <h4>1. Noise Reduction</h4>
            <p>
                - <strong>SmoothGrad:</strong> Explicitly reduces gradient noise by averaging gradients over multiple noisy inputs. This results in smoother and more stable attribution maps.
            </p>
            <p>
                - <strong>Expected Gradients:</strong> While it averages over reference inputs to approximate Shapley values, it may still retain some noise inherent in gradient estimates.
            </p>

            <h4>2. Computational Efficiency</h4>
            <p>
                - <strong>SmoothGrad:</strong> Typically requires fewer samples to achieve significant noise reduction, making it more computationally efficient.
            </p>
            <p>
                - <strong>Expected Gradients:</strong> May require a larger number of samples from the background dataset to accurately approximate Shapley values, increasing computational overhead.
            </p>

            <h4>3. Interpretability</h4>
            <p>
                - <strong>SmoothGrad:</strong> Produces clearer and more visually interpretable attribution maps, aiding in better understanding of model decisions.
            </p>
            <p>
                - <strong>Expected Gradients:</strong> Although theoretically aligned with Shapley values, the resulting attributions may be less intuitive due to residual noise.
            </p>

            <h4>4. Flexibility with Baseline Inputs</h4>
            <p>
                - <strong>SmoothGrad:</strong> Does not rely on a single baseline input, instead uses noisy samples, providing a more robust explanation across varied input perturbations.
            </p>
            <p>
                - <strong>Expected Gradients:</strong> Depends on the choice of baseline and the background dataset, which can influence the stability and reliability of attributions.
            </p>
        </section>

        <section>
            <h3>Conclusion</h3>
            <p>
                Through comprehensive analysis and empirical evaluation, this research demonstrates that <strong>SmoothGrad</strong> offers superior performance compared to <strong>Expected Gradients</strong> in the realm of Shapley value-based feature attribution. The enhanced noise reduction, computational efficiency, and improved interpretability of SmoothGrad make it a more effective tool for understanding and interpreting deep learning model predictions.
            </p>
            <p>
                These findings contribute to the ongoing efforts to develop more reliable and user-friendly interpretability methods, ultimately fostering greater trust and transparency in machine learning models.
            </p>
        </section>
    </div>

    <!-- Footer -->
    <footer>
        &copy; 2024 Your Name. All rights reserved.
    </footer>

</body>
</html>
